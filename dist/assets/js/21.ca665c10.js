(window.webpackJsonp=window.webpackJsonp||[]).push([[21],{417:function(t,s,a){t.exports=a.p+"assets/img/image-20240516151411101.5dfb0102.png"},758:function(t,s,a){"use strict";a.r(s);var e=a(7),n=Object(e.a)({},(function(){var t=this,s=t._self._c;return s("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[s("p",[t._v("opentelemetry提供了一种类似于 filebeat 的日志收集插件："),s("strong",[t._v("filelogreceiver")])]),t._v(" "),s("blockquote",[s("p",[t._v("官方说明：https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/receiver/filelogreceiver")])]),t._v(" "),s("p",[t._v("这种就是通用的日志收集方案，它和语言无关，opentelemetry 只关心日志在哪里，具体怎么收集，正则？单行？改写？过滤？这些能力都支持。")]),t._v(" "),s("p",[t._v("简单来说，你只需要配置"),s("strong",[t._v("日志路径")]),t._v("即可。")]),t._v(" "),s("p",[t._v("一般来说，常用4种日志切割方案：")]),t._v(" "),s("ul",[s("li",[t._v("单行全文 （默认分行符号切割）")]),t._v(" "),s("li",[t._v("多行全文 （需要行首正则切割）")]),t._v(" "),s("li",[t._v("多行正则（正则切割）")])]),t._v(" "),s("h2",{attrs:{id:"单行全文"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#单行全文"}},[t._v("#")]),t._v(" 单行全文")]),t._v(" "),s("p",[t._v("单行全文最简单了，只需要配置日子里路径即可：")]),t._v(" "),s("div",{staticClass:"language-yaml extra-class"},[s("pre",{pre:!0,attrs:{class:"language-yaml"}},[s("code",[s("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("filelog")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n  "),s("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("'include'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n  "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("-")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'/otel-file-log/data/log/app/*.log'")]),t._v("\n  "),s("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("'include_file_path'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token tag"}},[t._v("!!bool")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'true'")]),t._v("\n  "),s("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("'operators'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n")])])]),s("p",[t._v("单行全文 需要注意的是，这里的时间是 filelogreceiver 日志收集的时间，并不是真实的日志打印时间。")]),t._v(" "),s("p",[t._v("同时，像Java、Python的堆栈报错日志，都是分行输出的，也会被采集成一条单独的记录。")]),t._v(" "),s("p",[t._v("我们其实更需要的是 日志真正记录的时间以及分行合并。")]),t._v(" "),s("h2",{attrs:{id:"多行正则"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#多行正则"}},[t._v("#")]),t._v(" 多行正则")]),t._v(" "),s("p",[t._v("多行正则就不用担心日志时间这个问题，我们可以通过正则，把日志真正记录的时间给提取出来。")]),t._v(" "),s("p",[t._v("假如我本地的日志如下：")]),t._v(" "),s("div",{staticClass:"language-verilog extra-class"},[s("pre",{pre:!0,attrs:{class:"language-verilog"}},[s("code",[s("span",{pre:!0,attrs:{class:"token number"}},[t._v("2024")]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("05")]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("15")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("16")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("19")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("51")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("823")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("WARNING"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("6")]),t._v("de580dbb64922369c7a359fef697e25"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("166132")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("MainThread"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("otel_sdk_config"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("py  "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("125")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("use")]),t._v(" NoOpTracerProvider\nTraceback "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("most recent call last"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n  File "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"G:\\PyCharm 2023.3.4\\plugins\\python\\helpers\\pydev\\_pydevd_bundle\\pydevd_pep_669_tracing.py"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" line "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("111")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" in __call__\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("not")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("is_thread_alive")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("thread"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n           "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("~~~~~~~~~~~~~~~^^^^^^^^")]),t._v("\n")])])]),s("p",[t._v("借用正则平台https://regex101.com ，测试一下正则表达式：")]),t._v(" "),s("p",[s("img",{attrs:{src:a(417),alt:"./"}})]),t._v(" "),s("p",[t._v("然后把正确的正则表达式复制到配置这里，我本地 collector sidecar 的 "),s("code",[t._v("config.yaml")]),t._v(" 配置如下：")]),t._v(" "),s("div",{staticClass:"language-yaml extra-class"},[s("pre",{pre:!0,attrs:{class:"language-yaml"}},[s("code",[s("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("receivers")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n  "),s("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("filelog")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("start_at")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" beginning\n    "),s("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("include")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("-")]),t._v(" /otel"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("-")]),t._v("file"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("-")]),t._v("log/data/log/app/"),s("span",{pre:!0,attrs:{class:"token important"}},[t._v("*.log")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("include_file_path")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token boolean important"}},[t._v("true")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("multiline")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n      "),s("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("line_start_pattern")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2},\\d+'")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("operators")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("-")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("regex")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'(?P<log_time>\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2},\\d+) \\[(?P<log_level>\\w+)\\s*\\] \\[(?P<trace_id>\\w*)\\] - \\[(?P<process_id>\\d+)\\] - \\[(?P<thread_name>\\S+)\\] - \\[(?P<class_name>\\S+)\\s*\\]\\[(?P<line>\\d+)\\s*\\] : (?P<msg>[\\s\\S]*)'")]),t._v("\n      "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 使用正则切割的 日志level 替代")]),t._v("\n      "),s("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("severity")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("parse_from")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" attributes.log_level\n      "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 使用正则切割的 log_time 替掉 ")]),t._v("\n      "),s("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("timestamp")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("layout")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'%Y-%m-%d %H:%M:%S,%L'")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("layout_type")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" strptime\n        "),s("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("parse_from")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" attributes.log_time\n      "),s("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("type")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" regex_parser\n   "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("-")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("field")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" attributes.log_time\n     "),s("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("type")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" remove\n   "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("-")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("field")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" attributes.log_level\n     "),s("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("type")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" remove\n")])])])])}),[],!1,null,null,null);s.default=n.exports}}]);